{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright (C) 2010 Radim Rehurek <radimrehurek@seznam.cz>\n",
    "# Licensed under the GNU LGPL v2.1 - http://www.gnu.org/licenses/lgpl.html\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This module implements the concept of Dictionary -- a mapping between words and\n",
    "their integer ids.\n",
    "\n",
    "Dictionaries can be created from a corpus and can later be pruned according to\n",
    "document frequency (removing (un)common words via the :func:`Dictionary.filter_extremes` method),\n",
    "save/loaded from disk (via :func:`Dictionary.save` and :func:`Dictionary.load` methods) etc.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import with_statement\n",
    "\n",
    "import logging\n",
    "import itertools\n",
    "import UserDict\n",
    "\n",
    "from gensim import utils\n",
    "\n",
    "\n",
    "logger = logging.getLogger('gensim.corpora.dictionary')\n",
    "\n",
    "\n",
    "class Dictionary(utils.SaveLoad, UserDict.DictMixin):\n",
    "    \"\"\"\n",
    "    Dictionary encapsulates the mapping between normalized words and their integer ids.\n",
    "\n",
    "    The main function is `doc2bow`, which converts a collection of words to its\n",
    "    bag-of-words representation: a list of (word_id, word_frequency) 2-tuples.\n",
    "    \"\"\"\n",
    "    def __init__(self, documents=None):\n",
    "        self.token2id = {} # token -> tokenId\n",
    "        self.id2token = {} # reverse mapping for token2id; only formed on request, to save memory\n",
    "        self.dfs = {} # document frequencies: tokenId -> in how many documents this token appeared\n",
    "\n",
    "        self.num_docs = 0 # number of documents processed\n",
    "        self.num_pos = 0 # total number of corpus positions\n",
    "        self.num_nnz = 0 # total number of non-zeroes in the BOW matrix\n",
    "\n",
    "        if documents:\n",
    "            self.add_documents(documents)\n",
    "\n",
    "\n",
    "    def __getitem__(self, tokenid):\n",
    "        if len(self.id2token) != len(self.token2id):\n",
    "            # the word->id mapping has changed (presumably via add_documents);\n",
    "            # recompute id->word accordingly\n",
    "            self.id2token = dict((v, k) for k, v in self.token2id.iteritems())\n",
    "        return self.id2token[tokenid] # will throw for non-existent ids\n",
    "\n",
    "\n",
    "    def keys(self):\n",
    "        \"\"\"Return a list of all token ids.\"\"\"\n",
    "        return self.token2id.values()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of token->id mappings in the dictionary.\n",
    "        \"\"\"\n",
    "        return len(self.token2id)\n",
    "\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\"Dictionary(%i unique tokens)\" % len(self))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def from_documents(documents):\n",
    "        return Dictionary(documents=documents)\n",
    "\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        \"\"\"\n",
    "        Build dictionary from a collection of documents. Each document is a list\n",
    "        of tokens = **tokenized and normalized** utf-8 encoded strings.\n",
    "\n",
    "        This is only a convenience wrapper for calling `doc2bow` on each document\n",
    "        with `allow_update=True`.\n",
    "\n",
    "        >>> print Dictionary([\"mรกma mele maso\".split(), \"ema mรก mรกma\".split()])\n",
    "        Dictionary(5 unique tokens)\n",
    "        \"\"\"\n",
    "        for docno, document in enumerate(documents):\n",
    "            if docno % 10000 == 0:\n",
    "                logger.info(\"adding document #%i to %s\" % (docno, self))\n",
    "            _ = self.doc2bow(document, allow_update=True) # ignore the result, here we only care about updating token ids\n",
    "        logger.info(\"built %s from %i documents (total %i corpus positions)\" %\n",
    "                     (self, self.num_docs, self.num_pos))\n",
    "\n",
    "\n",
    "    def doc2bow(self, document, allow_update=False, return_missing=False):\n",
    "        \"\"\"\n",
    "        Convert `document` (a list of words) into the bag-of-words format = list\n",
    "        of `(token_id, token_count)` 2-tuples. Each word is assumed to be a\n",
    "        **tokenized and normalized** utf-8 encoded string. No further preprocessing\n",
    "        is done on the words in `document`; apply tokenization, stemming etc. before\n",
    "        calling this method.\n",
    "\n",
    "        If `allow_update` is set, then also update dictionary in the process: create\n",
    "        ids for new words. At the same time, update document frequencies -- for\n",
    "        each word appearing in this document, increase its document frequency (`self.dfs`)\n",
    "        by one.\n",
    "\n",
    "        If `allow_update` is **not** set, this function is `const`, aka read-only.\n",
    "        \"\"\"\n",
    "        result = {}\n",
    "        missing = {}\n",
    "        document = sorted(document)\n",
    "        # construct (word, frequency) mapping. in python3 this is done simply\n",
    "        # using Counter(), but here i use itertools.groupby() for the job\n",
    "        for word_norm, group in itertools.groupby(document):\n",
    "            frequency = len(list(group)) # how many times does this word appear in the input document\n",
    "            tokenid = self.token2id.get(word_norm, None)\n",
    "            if tokenid is None:\n",
    "                # first time we see this token (~normalized form)\n",
    "                if return_missing:\n",
    "                    missing[word_norm] = frequency\n",
    "                if not allow_update: # if we aren't allowed to create new tokens, continue with the next unique token\n",
    "                    continue\n",
    "                tokenid = len(self.token2id)\n",
    "                self.token2id[word_norm] = tokenid # new id = number of ids made so far; NOTE this assumes there are no gaps in the id sequence!\n",
    "\n",
    "            # update how many times a token appeared in the document\n",
    "            result[tokenid] = frequency\n",
    "\n",
    "        if allow_update:\n",
    "            self.num_docs += 1\n",
    "            self.num_pos += len(document)\n",
    "            self.num_nnz += len(result)\n",
    "            # increase document count for each unique token that appeared in the document\n",
    "            for tokenid in result.iterkeys():\n",
    "                self.dfs[tokenid] = self.dfs.get(tokenid, 0) + 1\n",
    "\n",
    "        # return tokenids, in ascending id order\n",
    "        result = sorted(result.iteritems())\n",
    "        if return_missing:\n",
    "            return result, missing\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "\n",
    "    def filter_extremes(self, no_below=5, no_above=0.5, keep_n=100000):\n",
    "        \"\"\"\n",
    "        Filter out tokens that appear in\n",
    "\n",
    "        1. less than `no_below` documents (absolute number) or\n",
    "        2. more than `no_above` documents (fraction of total corpus size, *not*\n",
    "           absolute number).\n",
    "        3. after (1) and (2), keep only the first `keep_n` most frequent tokens (or\n",
    "           keep all if `None`).\n",
    "\n",
    "        After the pruning, shrink resulting gaps in word ids.\n",
    "\n",
    "        **Note**: Due to the gap shrinking, the same word may have a different\n",
    "        word id before and after the call to this function!\n",
    "        \"\"\"\n",
    "        no_above_abs = int(no_above * self.num_docs) # convert fractional threshold to absolute threshold\n",
    "\n",
    "        # determine which tokens to keep\n",
    "        good_ids = (v for v in self.token2id.itervalues() if no_below <= self.dfs[v] <= no_above_abs)\n",
    "        good_ids = sorted(good_ids, key=self.dfs.get, reverse=True)\n",
    "        if keep_n is not None:\n",
    "            good_ids = good_ids[:keep_n]\n",
    "        logger.info(\"keeping %i tokens which were in no less than %i and no more than %i (=%.1f%%) documents\" %\n",
    "                     (len(good_ids), no_below, no_above_abs, 100.0 * no_above))\n",
    "\n",
    "        # do the actual filtering, then rebuild dictionary to remove gaps in ids\n",
    "        self.filter_tokens(good_ids=good_ids)\n",
    "        self.compactify()\n",
    "        logger.info(\"resulting dictionary: %s\" % self)\n",
    "\n",
    "\n",
    "    def filter_tokens(self, bad_ids=None, good_ids=None):\n",
    "        \"\"\"\n",
    "        Remove the selected `bad_ids` tokens from all dictionary mappings, or, keep\n",
    "        selected `good_ids` in the mapping and remove the rest.\n",
    "\n",
    "        `bad_ids` and `good_ids` are collections of word ids to be removed.\n",
    "        \"\"\"\n",
    "        if bad_ids is not None:\n",
    "            bad_ids = set(bad_ids)\n",
    "            self.token2id = dict((token, tokenid) for token, tokenid in self.token2id.iteritems() if tokenid not in bad_ids)\n",
    "            self.dfs = dict((tokenid, freq) for tokenid, freq in self.dfs.iteritems() if tokenid not in bad_ids)\n",
    "        if good_ids is not None:\n",
    "            good_ids = set(good_ids)\n",
    "            self.token2id = dict((token, tokenid) for token, tokenid in self.token2id.iteritems() if tokenid in good_ids)\n",
    "            self.dfs = dict((tokenid, freq) for tokenid, freq in self.dfs.iteritems() if tokenid in good_ids)\n",
    "\n",
    "\n",
    "    def compactify(self):\n",
    "        \"\"\"\n",
    "        Assign new word ids to all words.\n",
    "\n",
    "        This is done to make the ids more compact, e.g. after some tokens have\n",
    "        been removed via :func:`filter_tokens` and there are gaps in the id series.\n",
    "        Calling this method will remove the gaps.\n",
    "        \"\"\"\n",
    "        logger.debug(\"rebuilding dictionary, shrinking gaps\")\n",
    "\n",
    "        # build mapping from old id -> new id\n",
    "        idmap = dict(itertools.izip(self.token2id.itervalues(), xrange(len(self.token2id))))\n",
    "\n",
    "        # reassign mappings to new ids\n",
    "        self.token2id = dict((token, idmap[tokenid]) for token, tokenid in self.token2id.iteritems())\n",
    "        self.dfs = dict((idmap[tokenid], freq) for tokenid, freq in self.dfs.iteritems())\n",
    "\n",
    "\n",
    "    def save_as_text(self, fname):\n",
    "        \"\"\"\n",
    "        Save this Dictionary to a text file, in format:\n",
    "        `id[TAB]word_utf8[TAB]document frequency[NEWLINE]`.\n",
    "\n",
    "        Note: use `save`/`load` to store in binary format instead (pickle).\n",
    "        \"\"\"\n",
    "        logger.info(\"saving dictionary mapping to %s\" % fname)\n",
    "        with open(fname, 'wb') as fout:\n",
    "            for token, tokenid in sorted(self.token2id.iteritems()):\n",
    "                fout.write(\"%i\\t%s\\t%i\\n\" % (tokenid, token, self.dfs[tokenid]))\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def load_from_text(fname):\n",
    "        \"\"\"\n",
    "        Load a previously stored Dictionary from a text file.\n",
    "        Mirror function to `save_as_text`.\n",
    "        \"\"\"\n",
    "        result = Dictionary()\n",
    "        with open(fname, 'rb') as f:\n",
    "            for lineno, line in enumerate(f):\n",
    "                try:\n",
    "                    wordid, word, docfreq = line[:-1].split('\\t')\n",
    "                except Exception:\n",
    "                    raise ValueError(\"invalid line in dictionary file %s: %s\"\n",
    "                                     % (fname, line.strip()))\n",
    "                wordid = int(wordid)\n",
    "                result.token2id[word] = wordid\n",
    "                result.dfs[wordid] = int(docfreq)\n",
    "        return result\n",
    "#endclass Dictionary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
